[model_specification]
model_type=llama_13b ; gptj_6b, llama_13b, llama_33b
data_type=fp16       ; fp32, bf16
max_concurrency=32    ; maximum requests for fusion before OOM.
per_batch_size=1     ; number of samples in each request
max_seq_len=512     ; maximum generation length of each request
fixed_input_len=8
fixed_prompt_len=0
beam_width=1
tensor_para_size=1
pipeline_para_size=1

[runtime_hyperparameter]
top_k=1 ; k value for top k sampling
top_p=0.0 ; p value for top p sampling
temperature=1.0 ; Use for sampling
repetition_penalty=1.0 ; Use for sampling
presence_penalty=0.0  ; Only one of repetition_penalty and presence_penalty are allowed.
len_penalty=0.0
beam_search_diversity_rate=0.0
data_type=fp16
enable_custom_all_reduce=0
interval=1000
use_mem_shuffle=1

[request]
request_output_len=504

[llama_6b]
head_num = 16
size_per_head = 256
inter_size = 16384
decoder_layers = 28
rotary_embedding = 64
vocab_size = 50400
start_id = 50256
end_id = 50256
weight_data_type = fp16
layernorm_eps=1e-06
model_dir=/scratch/yao.877/llama/llama-6b/convert
shared_contexts_ratio=1.0

[llama_7b]
head_num = 32
size_per_head = 128
inter_size = 11008
decoder_layers = 32
rotary_embedding = 128
vocab_size = 32000
start_id = 0
end_id = 1
weight_data_type = fp16
layernorm_eps=1e-06
model_dir=/scratch/yao.877/llama/llama-7b/convert
shared_contexts_ratio=1.0

[llama_13b]
head_num = 40
size_per_head = 128
inter_size = 13824
decoder_layers = 40
rotary_embedding = 128
vocab_size = 32000
start_id = 0
end_id = 1
weight_data_type = fp16
layernorm_eps=1e-06
model_dir=/scratch/yao.877/llama/llama-13b/convert
shared_contexts_ratio=1.0


[llama_33b]
head_num = 52
size_per_head = 128
inter_size = 17920
decoder_layers = 60
rotary_embedding = 128
vocab_size = 32000
start_id = 0
end_id = 1
weight_data_type = fp16
layernorm_eps=1e-06
model_dir=/scratch/yao.877/llama/llama-33b/convert
shared_contexts_ratio=1.0


[llama_65b]
head_num = 64
size_per_head = 128
inter_size = 11008
decoder_layers = 80
rotary_embedding = 128
vocab_size = 32000
start_id = 0
end_id = 1
weight_data_type = fp16
layernorm_eps=1e-06
model_dir=/scratch/yao.877/llama/llama-65b/convert
shared_contexts_ratio=1.0